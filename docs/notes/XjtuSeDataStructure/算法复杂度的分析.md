# 算法的复杂度分析

> 作者：李彦筱
>
> 感谢学长 [yijunquan](https://github.com/yijunquan-afk) 的[软件工程笔记](https://github.com/yijunquan-afk/XJTUSE-NOTES)给我的启发。此笔记从二叉树开始的部分中使用了学长笔记中的部分图片以及文字。
>

## 如何比较两种解决问题的算法的效率？

通过分析两个算法的代价，就可以分析它的效率。那怎么知道两个算法的代价呢？

### 方法一：简朴法

直接用程序实现这两个算法，然后用同一组数据运行，比较一下运行时间。

显而易见，这个方法有很大的弊端：我必须费很大的力气实现所有“可能能用”的算法，而且我实现了的所有算法里只有一种能用，其他的都浪费了。

而且，这种测试方法有失公允。如果这几种算法是用不同编程语言生成的，那么，它们从一开始运行速度就不可能一样；如果它们用了同一种编程语言实现，但是在不同机器上运行，那可能由于配置原因，运行速度不一样；或者，二者测试的数据即使相同，因为算法的实现不同，时间长度可能也不同

>  比如，顺序查找在查找位于第一个位置的元素肯定比二分查找快，但这就说明顺序查找的速度一定快于二分吗？未必

### 方法二：渐进分析法

估算当问题规模变大的时候，一种算法及其实现它程序所消耗时间的**增长速度**

就是不看某种特定情况下的速度，而是看问题规模的增长引起的每种算法**消耗资源的增长速度**。

现在广泛使用的就是这种方法。

## 算法消耗的资源

分析算法需要估算算法将会消耗的资源。一般来说，算法消耗的资源包括时间资源和空间资源。

### 时间代价

时间代价指的是该算法在特定输入上需要执行的基本操作次数。

测量时间代价的时候，应当排除由于运行环境不同（比如硬件好坏不同，编程语言不同，……）产生的时间不同。

### 空间代价

### 算法的最好/最坏/平均情况

很多算法处理同一规模下的不同输入时，消耗的运行时间**不同**。

**最佳情况**

顺序查找算法在查找元素位于第一位时最快；这种情况叫做**最佳情况**

一般不比较算法的最佳情况，因为发生的概率太小，而且没有意义。

**最差情形**

顺序查找在查找元素位于最后一位时最慢；这种情况叫做**最差情况**

算法的最差情况是**需要关注的**。它会告诉你算法**最少能做多快**

**平均情况**

假如待查找元素出现在数组每个元素的几率均等；那么，平均下来，找到每个元素的平均次数需要 (N+1)/2 次。

不过，大部分时候，算法的平均情况很难计算。这是因为算法的输入的分布概率不一定一致。

虽然平均情形相对“准确”一点，不过因为大部分时候都很难计算，一般考虑**算法的最差情形**

### 算法分析的模型

为了排除编程语言，不同的硬件之类的影响，我们定义一个能运行所有程序的计算机模型。

这台计算机：

1. 拥有简单的指令执行能力，比如加减乘除，比较，复制等；
2. 执行每一个指令**都恰好花费一个时间单位**
3. 拥有**无限多**的内存（为了忽略实际中磁盘与内存的IO对程序效率的影响）

这样计算出的算法的运行时间能作为判断算法质量的依据吗？

数据规模不大的时候：

- 运行时间函数中每一项的系数都影响了运行时间

数据规模不断增大时：

- 运行时间函数的运行速度大概只和高次项的系数有关
- 只有**具有相同高次项**的运行时间函数的增长速度一致。

因此，我们只关注每个算法运行时间函数的**最高次项**（因为低次项在数据规模增加时，增大速度远低于高次项），且**忽略其系数**（因为只考虑变化的差值，而不是绝对值；增长速度 n*a / n * a<sup>2</sup> 和 a / a<sup>2</sup> 没有任何区别）

### 数学基础（大O标记）

如果存在正整数 c, n<sub>0</sub> ，使得当 n>=n<sub>0</sub> 时，恒有 f(n) < c * g(n) （你的函数的结果在另一个函数的常数次倍内），那么称为 `f(n) = O(g(n))`

（念为 f 在 O(g(n)) 中）

O(g(n)) 表示了你的函数的增长速度不会超过函数 g(n) 。显然，如果一个函数满足 f(n) = O(n<sup>2</sup>)，那它一定满足 f(n) = O(n<sup>3</sup>)，因为 n<sup>3</sup> 比 n<sup>2</sup> 的增长速度快。不过，既然用 O 标记，一般都是为了找到函数的最紧的上界，因此应当选用 f(n) = O(n<sup>2</sup>)。

O表示法的目的是确定一个函数的上界。

#### 大 Omega 标记（Ω）

用于表示一个函数的**渐进下界**。如果存在正整数 c 和 n<sub>0</sub>，使得当 n>=n<sub>0</sub> 时，恒有 f(n) > c * g(n)（你的函数的结果在另一个函数的常数次倍以上），那么称为 `f(n) = Ω(g(n))`。

（念为 f 在 Ω(g(n)) 中）

Ω(g(n)) 表示了你的函数的增长速度不会慢于函数 g(n)。换句话说，函数 f(n) 的增长速度至少和 g(n) 一样快。类似于大 O 标记，如果一个函数满足 f(n) = Ω(n<sup>2</sup>)，那它一定满足 f(n) = Ω(n)，因为 n 的增长速度不会超过 n<sup>2</sup>。通常，大 Omega 标记用于确定一个函数的下界。

#### 大 Theta 标记（Θ）

表示一个函数的**紧确界**。如果一个函数既是大 O 又是大 Omega，即存在正整数 c1、c2、n<sub>0</sub>，使得当 n>=n<sub>0</sub> 时，满足 c1 * g(n) < f(n) < c2 * g(n)，那么称为 `f(n) = Θ(g(n))`。

（念为 f 在 Θ(g(n)) 中）

Θ(g(n)) 表示了函数 f(n) 的增长速度与 g(n) 相匹配。在算法分析中，大 Theta 标记用于给出一个函数的紧确增长界。如果一个函数 f(n) 既是 Θ(n<sup>2</sup>)，又是 Θ(n<sup>3</sup>)，那么我们可以说 f(n) 的增长速度是紧确界在 n<sup>2</sup> 和 n<sup>3</sup> 之间。

大 O 标记用于表示上界，大 Omega 标记用于表示下界，而大 Theta 标记则用于表示上下界的紧确关系。

#### 为什么使用大O标记？

因为前面说到，我们一般考虑算法的时候，会考虑算法最差的运行情况，而O反应算法运行速度的上界，和我们的需求一样。

### 大O标记的运算法则

1. 如果 f(n) = O(g(n))，而且 g(n) = O(h(n)) ,那么 f(n) = O(h(n))

   > 就好像“我上界的上界还是我的上界”

2. 若 f(n) = O(k*g(n)) 则 k>0 时，f(n) = O(g(n))

   > 定义里头能证明这个法则

3. 如果 f<sub>1</sub>(n) = O(g<sub>1</sub>(n))，f<sub>2</sub>(n) = O(g<sub>2</sub>(n))，那么 f<sub>1</sub>(n) + f<sub>2</sub>(n) = O(max(g<sub>1</sub>(n), g<sub>2</sub>(n)))

   > 比如：对于 f<sub>1</sub>(n) = n<sup>2</sup>, f<sub>2</sub>(n) = 2 * n, 显然 f<sub>1</sub>(n) = O(n<sup>2</sup>)，f<sub>2</sub>(n) = O(n)，
   >
   > f<sub>1</sub>(n) + f<sub>2</sub>(n) = n<sup>2</sup> + 2 * n = O(n<sup>2</sup>)，n<sup>2</sup> = max(n<sup>2</sup>, n)，证明法则是对的

4. 如果 f<sub>1</sub>(n) = O(g<sub>1</sub>(n))，f<sub>2</sub>(n) = O(g<sub>2</sub>(n))，那么 f<sub>1</sub>(n) * f<sub>2</sub>(n) = O(g<sub>1</sub>(n) * g<sub>2</sub>(n))

   > 比如：对于 f<sub>1</sub>(n) = n<sup>2</sup>, f<sub>2</sub>(n) = 2 * n, 显然 f<sub>1</sub>(n) = O(n<sup>2</sup>)，f<sub>2</sub>(n) = O(n)，
   >
   > f<sub>1</sub>(n) * f<sub>2</sub>(n) = n<sup>2</sup> * 2n = 2n<sup>3</sup> = O(n<sup>3</sup>)，n<sup>3</sup> = n<sup>2</sup> * n，证明法则是对的

5. 对任何的常数 K,log<sup>k</sup>(n) 在 O(n) 中

   > 即 log<sup>k</sup>(n) 的时间复杂度低于一次级（n）的。那么显然 log(n) 的时间复杂度低于 n 的。

### 常见的函数的 O(n) 的曲线

各种函数的增长速度：横轴为输入样本数量，纵轴为运行时间：

在输入样本数很大很大时：

O(n<sup>3</sup>) > O(n<sup>2</sup>) > O(nlog(n)) > O(n)

在输入样本量不算大（100以内）时：

O(n<sup>3</sup>) > O(nlog(n)) > O(n<sup>2</sup>) > O(n)

请注意 **O(n<sup>2</sup>) 在数据规模很小时速度优于 O(nlog(n))**

更全的排序：

**O(2<sup>N</sup>) > O(n<sup>3</sup>) > O(n<sup>2</sup>) > O(nlog(n)) > O(n) > O(log<sup>2</sup>(n)) > O(log(n)) > O(1)**

## 运行时间函数的估算

### 法则一：for 循环法则

一个 for 循环的运行时间函数**等于每次循环花费时间乘循环次数**

一般来说，

```java
for (int i=0;i<N;i++){
    a++;
}
```

这种不嵌套的 for 循环的时间复杂度为 O(N)。

### 法则二：嵌套 for 循环

嵌套 for 循环的运行时间等于**每个 for 循环花费时间的乘积**

比如，

```java
for (int i=0;i<N;i++)
    for (int j=0;j<N;j++)
```

的时间复杂度为 O(N<sup>2</sup>)。

这个法则是由上面的**第四个运算法则**得出的。

### 法则三：顺序语句的计算

将其中各个语句的执行时间加起来即可。

也就是说，一大堆顺序语句的时间复杂度**取决于时间复杂度最高的那个**

```java
for (int i=0;i<N;i++){
    
}
for (int j=0;j<N;j++){
    for (int k=0;k<N;k++){
        
    }
}
```

这段代码的时间复杂度为 **O(n<sup>2</sup>)**。因为其中的双层循环是时间复杂度最高的部分，直接取其时间复杂度即可。

### 法则四：分支语句的时间复杂度

分支语句的执行时间直接取**运行时间最长的分支需要的时间**。因为我们在考虑运行时间的最差情况，不是平均情况。



具有线性增长率（O(n)）的算法是让人满意的。这种算法耗时的增加仅仅与数据规模的一次成正比。

具有增长率 O(nlogn) 的算法也是可以接受的，因为它的增长率相比 O(n) 并不大。

## 降低时间复杂度

O(n<sup>3</sup>) 到 O(n<sup>2</sup>) 的时间复杂度缩减是较为简单的：去除一些冗余的循环即可

O(n<sup>2</sup>) 到 O(nlogn) 的时间复杂度缩减是有固定模式的：

一般采用**分治策略+递归**的思想，即可把双循环缩减到O(nlogn)的范围
O(nlogn) 到 O(n) 是非常困难的：没有固定的化简模式，需要非常精妙的思想才能完成化简。

## 换一台更快的计算机，还是换一种算法？

我们在遇到问题的时候，应当尝试优化算法，而不是换一台更好的计算机。

假设有一台计算机A和B，其中B的运行速度是A的10倍。假设A一个小时能执行10000个指令，B一个小时可以执行100000个指令。

| 时间复杂度 | A 上此算法处理数量 | B     | 提升倍数   |
| ---------- | ------------------ | ----- | ---------- |
| 10n        | 1000               | 10000 | 10         |
| 20n        | 500                | 5000  | 10         |
| 5nlogn     | 250                | 1842  | 7.37       |
| n<sup>2</sup>       | 31                 | 1000  | 3.16       |
| 2<sup>n</sup>       | 13                 | 16    | 一倍都不到 |

根据摩尔定律，平均下来，计算机的计算能力提升10倍需要5年。而这5年里，需要处理的数据量也一定会大幅度增加。因此，应当尽力优化算法，而不是尝试找一台更好的计算机，因为只更换计算机的话，如果算法的时间复杂度很高，总有一天你会找不到可以用的计算机了。

提醒：更换 `mid = (low + high) / 2` 为 `mid = low + (high - low) / 2` 。因为前者可能导致数据溢出。

## 空间代价的分析

空间消耗的考虑：

1. 固定空间的消耗：

   用于存储传入内容的变量，暂时记录变量，结构体等…

2. 可变空间的消耗

   递归时，需要在栈区存储之前调用中变量的信息，以便在函数返回后保证其余内容的正常执行。

   一般来说，可变空间的消耗仅包括递归对栈区的消耗

### 时间-空间权衡原则

一般来说，牺牲空间或其他可替代资源，都可以减小时间代价

**基于磁盘的时间-空间权衡原则**

一般来说，算法运行时**与磁盘的交互越少**，算法执行越快。

> 因为磁盘的读取和写入速度相比内存实在是太太太太太慢了，更别跟高速缓存啥的比了。
